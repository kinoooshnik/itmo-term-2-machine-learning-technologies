{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexander.semiletov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexander.semiletov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alexander.semiletov/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.\tDownload Alice in Wonderland by Lewis Carroll from Project Gutenberg's website http://www.gutenberg.org/files/11/11-0.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines in text - 3352\n"
     ]
    }
   ],
   "source": [
    "with open('11-0.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.readlines()\n",
    "text = text[54:-355]\n",
    "print(f\"Lines in text - {len(text)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.\tPerform any necessary preprocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in text - 134048\n"
     ]
    }
   ],
   "source": [
    "text = \" \".join([line.strip() for line in text])\n",
    "text = re.sub(\"[^A-Za-z\\s]\", \"\", text)\n",
    "text = re.sub(\"\\s+\", \" \", text)\n",
    "print(f\"Characters in text - {len(text)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapters in text - 12\n"
     ]
    }
   ],
   "source": [
    "chapters = [chapter.strip() for chapter in text.split(\"CHAPTER\") if chapter.strip() not in [\"\"]]\n",
    "print(f\"Chapters in text - {len(chapters)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "chapters = [chapter.lower() for chapter in chapters]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "chapters = [[lemmatizer.lemmatize(token) for token in TreebankWordTokenizer().tokenize(chapter)] for chapter in\n",
    "            chapters]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "chapters = [[word for word in chapter if not word in stop_words + [\"wa\"]] for chapter in chapters]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in chapters: \n",
      "{1: 976,\n",
      " 2: 1000,\n",
      " 3: 824,\n",
      " 4: 1224,\n",
      " 5: 1037,\n",
      " 6: 1218,\n",
      " 7: 1116,\n",
      " 8: 1175,\n",
      " 9: 1116,\n",
      " 10: 1014,\n",
      " 11: 899,\n",
      " 12: 996}\n"
     ]
    }
   ],
   "source": [
    "words_count = {i + 1: len(chapter) for i, chapter in enumerate(chapters)}\n",
    "print(\"Words in chapters: \")\n",
    "pprint(words_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.\tFind Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "chapters = [\" \".join(chapter) for chapter in chapters]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer()\n",
    "X_tfidf = tfidf_model.fit_transform(chapters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "id_to_word = dict(zip(tfidf_model.vocabulary_.values(), tfidf_model.vocabulary_.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "[('alice', 0.29615517945409664),\n",
      " ('little', 0.1645306552522759),\n",
      " ('bat', 0.162315221600624),\n",
      " ('door', 0.14664694458776423),\n",
      " ('key', 0.14338221364669432),\n",
      " ('eat', 0.13614724395895683),\n",
      " ('think', 0.12065581385166899),\n",
      " ('like', 0.12065581385166899),\n",
      " ('way', 0.12065581385166899),\n",
      " ('either', 0.11669763767910585)]\n",
      "Chapter 2\n",
      "[('mouse', 0.2945290373216973),\n",
      " ('alice', 0.24915185841865883),\n",
      " ('pool', 0.18093864706387253),\n",
      " ('little', 0.17648256637988333),\n",
      " ('im', 0.1575024176206621),\n",
      " ('swam', 0.1490655936220378),\n",
      " ('cat', 0.14726451866084864),\n",
      " ('dear', 0.14415518447613193),\n",
      " ('said', 0.12457592920932942),\n",
      " ('foot', 0.12115570586204778)]\n",
      "Chapter 3\n",
      "[('mouse', 0.38649334391012735),\n",
      " ('said', 0.35289572334457703),\n",
      " ('dodo', 0.3071858131423766),\n",
      " ('alice', 0.23872357755662565),\n",
      " ('prize', 0.17884353644474438),\n",
      " ('lory', 0.1535929065711883),\n",
      " ('dry', 0.1356772995621639),\n",
      " ('thimble', 0.11922902429649625),\n",
      " ('know', 0.1141721457879514),\n",
      " ('bird', 0.11042666968860781)]\n",
      "Chapter 4\n",
      "[('alice', 0.2628892728292309),\n",
      " ('bill', 0.20563319271700048),\n",
      " ('little', 0.20154844250241036),\n",
      " ('window', 0.20132426293201644),\n",
      " ('rabbit', 0.1822445657540622),\n",
      " ('puppy', 0.17615873006551436),\n",
      " ('glove', 0.12967471171337616),\n",
      " ('bottle', 0.12967471171337616),\n",
      " ('fan', 0.12967471171337616),\n",
      " ('one', 0.12268166065364108)]\n",
      "Chapter 5\n",
      "[('caterpillar', 0.4437999550801091),\n",
      " ('said', 0.4074072818553916),\n",
      " ('alice', 0.2742164397103597),\n",
      " ('serpent', 0.26999841205797087),\n",
      " ('pigeon', 0.26999841205797087),\n",
      " ('im', 0.1371537497338998),\n",
      " ('egg', 0.13499920602898544),\n",
      " ('youth', 0.13499920602898544),\n",
      " ('size', 0.10724667725365687),\n",
      " ('father', 0.09661573859003116)]\n",
      "Chapter 6\n",
      "[('said', 0.3503501246133237),\n",
      " ('alice', 0.32053309273133873),\n",
      " ('cat', 0.3172275565816268),\n",
      " ('footman', 0.256885851202286),\n",
      " ('baby', 0.20223184028282135),\n",
      " ('mad', 0.17864282008585314),\n",
      " ('pig', 0.16546241477685386),\n",
      " ('duchess', 0.15502701866435303),\n",
      " ('wow', 0.128442925601143),\n",
      " ('like', 0.11926812752793998)]\n",
      "Chapter 7\n",
      "[('hatter', 0.44192018674851247),\n",
      " ('dormouse', 0.40931497388423504),\n",
      " ('said', 0.36265481645777675),\n",
      " ('alice', 0.3073345902184549),\n",
      " ('hare', 0.25241844962250237),\n",
      " ('march', 0.25241844962250237),\n",
      " ('twinkle', 0.14121666323624274),\n",
      " ('time', 0.10449376067427465),\n",
      " ('tea', 0.09374064567392688),\n",
      " ('draw', 0.09095888308538556)]\n",
      "Chapter 8\n",
      "[('queen', 0.43777858507328293),\n",
      " ('said', 0.31424174555839796),\n",
      " ('alice', 0.28500995527389583),\n",
      " ('hedgehog', 0.20986979743441003),\n",
      " ('king', 0.20007085160995008),\n",
      " ('gardener', 0.16789583794752805),\n",
      " ('soldier', 0.14329346839951898),\n",
      " ('cat', 0.14254216610546253),\n",
      " ('five', 0.12616703394026055),\n",
      " ('procession', 0.12592187846064604)]\n",
      "Chapter 9\n",
      "[('turtle', 0.39649259625622396),\n",
      " ('said', 0.38420031020760365),\n",
      " ('mock', 0.3818076852837712),\n",
      " ('alice', 0.3167967470132872),\n",
      " ('gryphon', 0.26361687852423166),\n",
      " ('duchess', 0.1902441878797164),\n",
      " ('moral', 0.1742127295611257),\n",
      " ('queen', 0.15278061623703892),\n",
      " ('went', 0.08762463215261135),\n",
      " ('say', 0.07279872689923383)]\n",
      "Chapter 10\n",
      "[('turtle', 0.4085257819784356),\n",
      " ('mock', 0.3689910288837483),\n",
      " ('gryphon', 0.366683501329523),\n",
      " ('said', 0.27219605555542253),\n",
      " ('lobster', 0.243193447742961),\n",
      " ('dance', 0.22582248718989237),\n",
      " ('alice', 0.1754152358023834),\n",
      " ('soup', 0.15813901237874925),\n",
      " ('beautiful', 0.15813901237874925),\n",
      " ('join', 0.15633864497761782)]\n",
      "Chapter 11\n",
      "[('king', 0.40138039538731085),\n",
      " ('hatter', 0.36118541397096693),\n",
      " ('said', 0.3157784129395362),\n",
      " ('court', 0.292056002009076),\n",
      " ('dormouse', 0.2531152017411992),\n",
      " ('witness', 0.2267132834245724),\n",
      " ('alice', 0.12631136517581448),\n",
      " ('queen', 0.11503296713641505),\n",
      " ('juror', 0.1133566417122862),\n",
      " ('officer', 0.1133566417122862)]\n",
      "Chapter 12\n",
      "[('said', 0.45631759747675205),\n",
      " ('king', 0.3849285148535422),\n",
      " ('alice', 0.1968428851860499),\n",
      " ('jury', 0.19493306150222392),\n",
      " ('queen', 0.14486197682364518),\n",
      " ('sister', 0.13645314305155676),\n",
      " ('dream', 0.1324038830352555),\n",
      " ('would', 0.11631625033721131),\n",
      " ('slate', 0.11033656919604624),\n",
      " ('rabbit', 0.10633150706359894)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(chapters)):\n",
    "    print(f\"Chapter {i + 1}\")\n",
    "    top_words = sorted(\n",
    "        [(id_to_word[id_], tfidf) for id_, tfidf in zip(X_tfidf.toarray()[i].argsort(), sorted(X_tfidf.toarray()[i]))],\n",
    "        key=lambda x: x[1], reverse=True)\n",
    "    pprint(top_words[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.\tFind the Top 10 most used verbs in sentences with Alice. What does Alice do most often?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "with open('11-0.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.readlines()\n",
    "text = text[54:-355]\n",
    "text = \" \".join([line.strip() for line in text])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = tokenizer.tokenize(text)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words(\"english\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/973 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b253dfb89a1e4082a4d1d1cf8b0e3551"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(len(sentences))):\n",
    "    sentences[i] = re.sub(\"[^A-Za-z\\s]\", \"\", sentences[i])\n",
    "    sentences[i] = re.sub(\"\\s+\", \" \", sentences[i])\n",
    "    sentences[i] = sentences[i].lower()\n",
    "    sentences[i] = [lemmatizer.lemmatize(token) for token in TreebankWordTokenizer().tokenize(sentences[i])]\n",
    "    sentences[i] = [word for word in sentences[i] if not word in stop_words + [\"wa\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/973 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa687077faf94d67b02888d55553585e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alice_words = []\n",
    "for sentence in tqdm(sentences):\n",
    "    if \"alice\" in sentence:\n",
    "        alice_words.extend(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "alice_words = [word for word, tag in nltk.pos_tag(alice_words) if tag[0] == \"V\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[('said', 256),\n ('went', 41),\n ('thought', 35),\n ('say', 34),\n ('know', 33),\n ('looked', 30),\n ('got', 26),\n ('go', 26),\n ('began', 24),\n ('see', 22)]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(alice_words).most_common(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}